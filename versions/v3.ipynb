{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7794551,"sourceType":"datasetVersion","datasetId":4563116}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"class Data:\n    def __init__(self, data_dir=\"data/FB15k-237/\", reverse=False):\n        self.train_data = self.load_data(data_dir, \"train\", reverse=reverse)\n        self.valid_data = self.load_data(data_dir, \"valid\", reverse=reverse)\n        self.test_data = self.load_data(data_dir, \"test\", reverse=reverse)\n        self.data = self.train_data + self.valid_data + self.test_data\n        self.entities = self.get_entities(self.data)\n        self.relations = self.get_relations(self.data)\n        self.train_relations = self.get_relations(self.train_data)\n        self.valid_relations = self.get_relations(self.valid_data)\n        self.test_relations = self.get_relations(self.test_data)\n\n    @staticmethod\n    def load_data(data_dir, data_type=\"train\", reverse=False):\n        with open(\"%s%s.txt\" % (data_dir, data_type), \"r\") as f:\n            data = f.read().strip().split(\"\\n\")\n            data = [i.split() for i in data]\n            if reverse:\n                data += [[i[2], i[1] + \"_reverse\", i[0]] for i in data]\n        return data\n\n    @staticmethod\n    def get_relations(data):\n        relations = sorted(list(set([d[1] for d in data])))\n        return relations\n\n    @staticmethod\n    def get_entities(data):\n        entities = sorted(list(set([d[0] for d in data] + [d[2] for d in data])))\n        return entities\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-08T16:25:11.966636Z","iopub.execute_input":"2024-03-08T16:25:11.967661Z","iopub.status.idle":"2024-03-08T16:25:11.978880Z","shell.execute_reply.started":"2024-03-08T16:25:11.967612Z","shell.execute_reply":"2024-03-08T16:25:11.978017Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"! pip3 install wandb","metadata":{"execution":{"iopub.status.busy":"2024-03-08T16:27:57.647670Z","iopub.execute_input":"2024-03-08T16:27:57.648066Z","iopub.status.idle":"2024-03-08T16:28:09.923560Z","shell.execute_reply.started":"2024-03-08T16:27:57.648037Z","shell.execute_reply":"2024-03-08T16:28:09.922455Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.3)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.40.5)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\n\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T16:28:27.566720Z","iopub.execute_input":"2024-03-08T16:28:27.567145Z","iopub.status.idle":"2024-03-08T16:28:39.601436Z","shell.execute_reply.started":"2024-03-08T16:28:27.567115Z","shell.execute_reply":"2024-03-08T16:28:39.600514Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"wandb.init(\n    # set the wandb project where this run will be logged\n    project=\"v3\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T18:28:19.337280Z","iopub.execute_input":"2024-03-08T18:28:19.337626Z","iopub.status.idle":"2024-03-08T18:28:54.379258Z","shell.execute_reply.started":"2024-03-08T18:28:19.337599Z","shell.execute_reply":"2024-03-08T18:28:54.378065Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:ozatkkcz) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Hits @1</td><td>▁▂▂▃▃▃▄▄▄▅▅▅▆▆▇▇██</td></tr><tr><td>Hits @10</td><td>▁▂▃▄▅▅▅▆▆▆▇▇▇▇████</td></tr><tr><td>Hits @3</td><td>▁▂▃▄▄▄▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>Mean reciprocal rank</td><td>▁▂▃▄▄▄▅▅▅▆▆▆▆▇▇███</td></tr><tr><td>T norm</td><td>▁▂▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>train_mean_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step_loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Hits @1</td><td>0.09438</td></tr><tr><td>Hits @10</td><td>0.25012</td></tr><tr><td>Hits @3</td><td>0.15279</td></tr><tr><td>Mean reciprocal rank</td><td>0.14616</td></tr><tr><td>T norm</td><td>371591.96875</td></tr><tr><td>train_mean_loss</td><td>0.00448</td></tr><tr><td>train_step_loss</td><td>0.00433</td></tr><tr><td>val_loss</td><td>0.02876</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">serene-morning-2</strong> at: <a href='https://wandb.ai/sudakov/v2/runs/ozatkkcz' target=\"_blank\">https://wandb.ai/sudakov/v2/runs/ozatkkcz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240308_174721-ozatkkcz/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:ozatkkcz). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240308_182819-t2zt3t7l</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sudakov/v3/runs/t2zt3t7l' target=\"_blank\">generous-thunder-1</a></strong> to <a href='https://wandb.ai/sudakov/v3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sudakov/v3' target=\"_blank\">https://wandb.ai/sudakov/v3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sudakov/v3/runs/t2zt3t7l' target=\"_blank\">https://wandb.ai/sudakov/v3/runs/t2zt3t7l</a>"},"metadata":{}},{"execution_count":41,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sudakov/v3/runs/t2zt3t7l?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x78f992b76680>"},"metadata":{}}]},{"cell_type":"code","source":"! pip3 install tucker_riemopt","metadata":{"execution":{"iopub.status.busy":"2024-03-08T17:47:58.619537Z","iopub.execute_input":"2024-03-08T17:47:58.620508Z","iopub.status.idle":"2024-03-08T17:48:11.223310Z","shell.execute_reply.started":"2024-03-08T17:47:58.620463Z","shell.execute_reply":"2024-03-08T17:48:11.222162Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tucker_riemopt in /opt/conda/lib/python3.10/site-packages (1.0.1)\nRequirement already satisfied: numpy<2.0.0,>=1.21.4 in /opt/conda/lib/python3.10/site-packages (from tucker_riemopt) (1.26.4)\nRequirement already satisfied: opt-einsum<4.0.0,>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from tucker_riemopt) (3.3.0)\nRequirement already satisfied: scipy<2.0.0,>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from tucker_riemopt) (1.11.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch import nn\nfrom torch.nn.init import xavier_normal_\nfrom tucker_riemopt import SFTucker\n\nfrom torch.optim import Optimizer\nfrom tucker_riemopt import SFTuckerRiemannian","metadata":{"execution":{"iopub.status.busy":"2024-03-08T17:48:11.225678Z","iopub.execute_input":"2024-03-08T17:48:11.226074Z","iopub.status.idle":"2024-03-08T17:48:11.233040Z","shell.execute_reply.started":"2024-03-08T17:48:11.226035Z","shell.execute_reply":"2024-03-08T17:48:11.231775Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"class SFTuckER:\n    def __init__(self, d, d1, d2):\n        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        self.rank = (d2, d1, d1)\n        self.E = torch.rand((len(d.entities), d1), device=device)\n        self.R = torch.rand((len(d.relations), d2), device=device)\n        self.W = torch.tensor(np.random.uniform(-1, 1, (d2, d1, d1)), dtype=torch.float, device=device)\n        \n    def parameters(self):\n        return nn.ParameterList([self.W, self.E, self.R])\n\n    def init(self):\n        xavier_normal_(self.E.data)\n        xavier_normal_(self.R.data)\n        # with torch.no_grad():\n        #     self.E.weight.data = torch.linalg.qr(self.E.weight)[0]\n        #     self.R.weight.data = torch.linalg.qr(self.R.weight)[0]\n\n    def forward(self, e_idx, r_idx):\n        relations = self.R[r_idx, :]\n        subjects = self.E[e_idx, :]\n        preds = torch.einsum(\"abc,da->dbc\", self.W, relations)\n        preds = torch.bmm(subjects.view(-1, 1, subjects.shape[1]), preds).view(-1, subjects.shape[1])\n        preds = preds @ self.E.T\n        return torch.sigmoid(preds)\n\n\nclass RGD(Optimizer):\n    def __init__(self, model_parameters, rank, max_lr):\n        self.rank = rank\n        self.max_lr = max_lr\n        self.lr = max_lr\n        self.direction = None\n        self.loss = None\n\n        defaults = dict(rank=rank, max_lr=self.max_lr, lr=self.lr)\n        params = model_parameters\n        super().__init__(params, defaults)\n\n    def fit(self, loss_fn, model, normalize_grad=False):\n        x_k = SFTucker(model.W.data, [model.R.data], num_shared_factors=2, shared_factor=model.E.data)\n        rgrad, self.loss = SFTuckerRiemannian.grad(loss_fn, x_k)\n        rgrad_norm = rgrad.norm().detach()\n\n        if normalize_grad:\n            normalizer = normalize_grad / rgrad_norm\n        else:\n            normalizer = 1\n\n        self.direction = normalizer * rgrad\n        return rgrad_norm\n\n    @torch.no_grad()\n    def step(self):\n        W, E, R = self.param_groups[0][\"params\"]\n\n        x_k = self.direction.point\n        x_k = (-self.param_groups[0][\"lr\"]) * self.direction + SFTuckerRiemannian.TangentVector(x_k)\n        x_k = x_k.construct().round(self.rank)\n\n        W.data.add_(x_k.core - W)\n        R.data.add_(x_k.regular_factors[0] - R)\n        E.data.add_(x_k.shared_factor - E)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T18:29:03.094350Z","iopub.execute_input":"2024-03-08T18:29:03.095240Z","iopub.status.idle":"2024-03-08T18:29:03.112826Z","shell.execute_reply.started":"2024-03-08T18:29:03.095207Z","shell.execute_reply":"2024-03-08T18:29:03.111858Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport time\nfrom collections import defaultdict\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport argparse\n\n\ndef get_loss_fn(e_idx, r_idx, targets, criterion, C = 0):\n    def loss_fn(T: SFTucker):\n        relations = T.regular_factors[0][r_idx, :]\n        subjects = T.shared_factor[e_idx, :]\n        preds = torch.einsum(\"abc,da->dbc\", T.core, relations)\n        preds = torch.bmm(subjects.view(-1, 1, subjects.shape[1]), preds).view(-1, subjects.shape[1])\n        preds = preds @ T.shared_factor.T\n        wandb.log({'T norm': T.norm()})\n        return criterion(torch.sigmoid(preds), targets) + C * T.norm()\n\n    return loss_fn\n\n\nclass Experiment:\n    def __init__(self, learning_rate=0.0005, ent_vec_dim=200, rel_vec_dim=200,\n                 num_iterations=500, batch_size=1024, decay_rate=0., label_smoothing=0.):\n        self.learning_rate = learning_rate\n        self.ent_vec_dim = ent_vec_dim\n        self.rel_vec_dim = rel_vec_dim\n        self.num_iterations = num_iterations\n        self.batch_size = batch_size\n        self.decay_rate = decay_rate\n        self.label_smoothing = label_smoothing\n        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        self.criterion = torch.nn.BCELoss()\n\n    def get_data_idxs(self, data):\n        data_idxs = [(self.entity_idxs[data[i][0]], self.relation_idxs[data[i][1]], self.entity_idxs[data[i][2]]) for i\n                     in range(len(data))]\n        return data_idxs\n\n    def get_er_vocab(self, data):\n        er_vocab = defaultdict(list)\n        for triple in data:\n            er_vocab[(triple[0], triple[1])].append(triple[2])\n        return er_vocab\n\n    def get_batch(self, er_vocab, er_vocab_pairs, idx):\n        batch = er_vocab_pairs[idx:idx + self.batch_size]\n        targets = np.zeros((len(batch), len(d.entities)))\n        for idx, pair in enumerate(batch):\n            targets[idx, er_vocab[pair]] = 1.\n        targets = torch.FloatTensor(targets).to(self.device)\n        return np.array(batch), targets\n\n    def evaluate(self, model, data):\n        hits = []\n        ranks = []\n        for i in range(10):\n            hits.append([])\n\n        test_data_idxs = self.get_data_idxs(data)\n        er_vocab = self.get_er_vocab(self.get_data_idxs(d.data))\n\n        losses = []\n        np.random.shuffle(test_data_idxs)\n        for i in range(0, len(test_data_idxs), self.batch_size):\n            data_batch, targets = self.get_batch(er_vocab, test_data_idxs, i)\n            e1_idx = torch.tensor(data_batch[:, 0]).to(self.device)\n            r_idx = torch.tensor(data_batch[:, 1]).to(self.device)\n            e2_idx = torch.tensor(data_batch[:, 2]).to(self.device)\n\n            targets = ((1.0 - self.label_smoothing) * targets) + (1.0 / targets.size(1))\n\n            predictions = model.forward(e1_idx, r_idx)\n\n            losses.append(self.criterion(predictions, targets).item())\n\n            for j in range(data_batch.shape[0]):\n                filt = er_vocab[(data_batch[j][0], data_batch[j][1])]\n                target_value = predictions[j, e2_idx[j]].item()\n                predictions[j, filt] = 0.0\n                predictions[j, e2_idx[j]] = target_value\n\n            sort_values, sort_idxs = torch.sort(predictions, dim=1, descending=True)\n\n            sort_idxs = sort_idxs.cpu().numpy()\n            for j in range(data_batch.shape[0]):\n                rank = np.where(sort_idxs[j] == e2_idx[j].item())[0][0]\n                ranks.append(rank + 1)\n\n                for hits_level in range(10):\n                    if rank <= hits_level:\n                        hits[hits_level].append(1.0)\n                    else:\n                        hits[hits_level].append(0.0)\n\n        wandb.log({'val_loss': np.mean(losses)})\n        wandb.log({'Hits @10': np.mean(hits[9])})\n        wandb.log({'Hits @3': np.mean(hits[2])})\n        wandb.log({'Hits @1': np.mean(hits[0])})\n        wandb.log({'Mean reciprocal rank': np.mean(1. / np.array(ranks))})\n#         print('val_loss:', np.mean(losses))\n#         print('Hits @10: {0}'.format(np.mean(hits[9])))\n#         print('Hits @3: {0}'.format(np.mean(hits[2])))\n#         print('Hits @1: {0}'.format(np.mean(hits[0])))\n#         print('Mean reciprocal rank: {0}'.format(np.mean(1. / np.array(ranks))))\n\n    def train_and_eval(self):\n        print(\"Training the TuckER model...\")\n        self.entity_idxs = {d.entities[i]: i for i in range(len(d.entities))}\n        self.relation_idxs = {d.relations[i]: i for i in range(len(d.relations))}\n\n        train_data_idxs = self.get_data_idxs(d.train_data)\n        print(\"Number of training data points: %d\" % len(train_data_idxs))\n\n        model = SFTuckER(d, self.ent_vec_dim, self.rel_vec_dim)\n\n        model.init()\n\n        opt = RGD(model.parameters(), (self.rel_vec_dim, self.ent_vec_dim, self.ent_vec_dim), self.learning_rate)\n        if self.decay_rate:\n            scheduler = ExponentialLR(opt, self.decay_rate)\n\n        er_vocab = self.get_er_vocab(train_data_idxs)\n        er_vocab_pairs = list(er_vocab.keys())\n\n        print(\"Starting training...\")\n        for it in range(1, self.num_iterations + 1):\n            print('Epoch:', it)\n            start_train = time.time()\n            losses = []\n            np.random.shuffle(er_vocab_pairs)\n            for j in range(0, len(er_vocab_pairs), self.batch_size):\n                data_batch, targets = self.get_batch(er_vocab, er_vocab_pairs, j)\n                opt.zero_grad()\n                e1_idx = torch.tensor(data_batch[:, 0]).to(self.device)\n                r_idx = torch.tensor(data_batch[:, 1]).to(self.device)\n\n                targets = ((1.0 - self.label_smoothing) * targets) + (1.0 / targets.size(1))\n\n                loss_fn = get_loss_fn(e1_idx, r_idx, targets, self.criterion)\n                grad_norm = opt.fit(loss_fn, model)\n                opt.step()\n                opt.zero_grad(set_to_none=True)\n\n                loss = opt.loss.detach()\n                wandb.log({'train_step_loss': loss.item()})\n#                 print(j / self.batch_size, loss)\n\n                losses.append(loss.item())\n            if self.decay_rate:\n                scheduler.step()\n#             print('time:', time.time() - start_train)\n            wandb.log({'train_mean_loss': np.mean(losses)})\n#             print(np.mean(losses))\n            with torch.no_grad():\n#                 print(\"Validation:\")\n                self.evaluate(model, d.valid_data)\n#                 if it % 5 == 0:\n#                     print(\"Test:\")\n#                     self.evaluate(model, d.test_data)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T18:29:04.455310Z","iopub.execute_input":"2024-03-08T18:29:04.455814Z","iopub.status.idle":"2024-03-08T18:29:04.491450Z","shell.execute_reply.started":"2024-03-08T18:29:04.455763Z","shell.execute_reply":"2024-03-08T18:29:04.490508Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"dataset = \"FB15k-237\"\nnum_iterations = 500\nbatch_size = 2048\nlr = 1e9\ndr = 1.0\nedim = 200\nrdim = 200\nlabel_smoothing = 0.1\n\ndata_dir = \"/kaggle/input/tucker-data/TuckER_data/%s/\" % dataset\ntorch.backends.cudnn.deterministic = True\nseed = 20\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\nd = Data(data_dir=data_dir, reverse=True)\nexperiment = Experiment(num_iterations=num_iterations, batch_size=batch_size, learning_rate=lr,\n                        decay_rate=dr, ent_vec_dim=edim, rel_vec_dim=rdim, label_smoothing=label_smoothing)\nexperiment.train_and_eval()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T18:29:05.586974Z","iopub.execute_input":"2024-03-08T18:29:05.587325Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training the TuckER model...\nNumber of training data points: 544230\nStarting training...\nEpoch: 1\nEpoch: 2\nEpoch: 3\nEpoch: 4\nEpoch: 5\nEpoch: 6\nEpoch: 7\nEpoch: 8\nEpoch: 9\nEpoch: 10\nEpoch: 11\nEpoch: 12\nEpoch: 13\nEpoch: 14\nEpoch: 15\nEpoch: 16\nEpoch: 17\nEpoch: 18\nEpoch: 19\nEpoch: 20\nEpoch: 21\nEpoch: 22\nEpoch: 23\nEpoch: 24\nEpoch: 25\nEpoch: 26\nEpoch: 27\nEpoch: 28\nEpoch: 29\nEpoch: 30\nEpoch: 31\nEpoch: 32\nEpoch: 33\nEpoch: 34\nEpoch: 35\nEpoch: 36\nEpoch: 37\nEpoch: 38\nEpoch: 39\nEpoch: 40\nEpoch: 41\nEpoch: 42\nEpoch: 43\nEpoch: 44\nEpoch: 45\nEpoch: 46\nEpoch: 47\nEpoch: 48\nEpoch: 49\nEpoch: 50\nEpoch: 51\nEpoch: 52\nEpoch: 53\nEpoch: 54\nEpoch: 55\nEpoch: 56\nEpoch: 57\nEpoch: 58\nEpoch: 59\nEpoch: 60\nEpoch: 61\nEpoch: 62\nEpoch: 63\nEpoch: 64\nEpoch: 65\nEpoch: 66\nEpoch: 67\nEpoch: 68\nEpoch: 69\nEpoch: 70\nEpoch: 71\nEpoch: 72\nEpoch: 73\nEpoch: 74\nEpoch: 75\nEpoch: 76\nEpoch: 77\nEpoch: 78\nEpoch: 79\nEpoch: 80\nEpoch: 81\nEpoch: 82\nEpoch: 83\nEpoch: 84\nEpoch: 85\nEpoch: 86\nEpoch: 87\nEpoch: 88\nEpoch: 89\nEpoch: 90\nEpoch: 91\nEpoch: 92\nEpoch: 93\nEpoch: 94\nEpoch: 95\nEpoch: 96\nEpoch: 97\nEpoch: 98\nEpoch: 99\nEpoch: 100\nEpoch: 101\nEpoch: 102\nEpoch: 103\nEpoch: 104\nEpoch: 105\nEpoch: 106\nEpoch: 107\nEpoch: 108\nEpoch: 109\nEpoch: 110\nEpoch: 111\nEpoch: 112\nEpoch: 113\nEpoch: 114\nEpoch: 115\nEpoch: 116\nEpoch: 117\nEpoch: 118\nEpoch: 119\nEpoch: 120\nEpoch: 121\nEpoch: 122\nEpoch: 123\nEpoch: 124\nEpoch: 125\nEpoch: 126\nEpoch: 127\nEpoch: 128\nEpoch: 129\nEpoch: 130\nEpoch: 131\nEpoch: 132\nEpoch: 133\nEpoch: 134\nEpoch: 135\nEpoch: 136\nEpoch: 137\nEpoch: 138\nEpoch: 139\nEpoch: 140\nEpoch: 141\nEpoch: 142\nEpoch: 143\nEpoch: 144\nEpoch: 145\nEpoch: 146\nEpoch: 147\nEpoch: 148\nEpoch: 149\nEpoch: 150\nEpoch: 151\nEpoch: 152\nEpoch: 153\nEpoch: 154\nEpoch: 155\nEpoch: 156\nEpoch: 157\nEpoch: 158\nEpoch: 159\nEpoch: 160\nEpoch: 161\nEpoch: 162\nEpoch: 163\nEpoch: 164\nEpoch: 165\nEpoch: 166\nEpoch: 167\nEpoch: 168\nEpoch: 169\nEpoch: 170\nEpoch: 171\nEpoch: 172\nEpoch: 173\nEpoch: 174\nEpoch: 175\nEpoch: 176\nEpoch: 177\nEpoch: 178\nEpoch: 179\nEpoch: 180\nEpoch: 181\nEpoch: 182\nEpoch: 183\nEpoch: 184\nEpoch: 185\nEpoch: 186\nEpoch: 187\nEpoch: 188\nEpoch: 189\nEpoch: 190\nEpoch: 191\nEpoch: 192\nEpoch: 193\nEpoch: 194\nEpoch: 195\nEpoch: 196\nEpoch: 197\nEpoch: 198\nEpoch: 199\nEpoch: 200\nEpoch: 201\nEpoch: 202\nEpoch: 203\nEpoch: 204\nEpoch: 205\nEpoch: 206\nEpoch: 207\nEpoch: 208\nEpoch: 209\nEpoch: 210\nEpoch: 211\nEpoch: 212\nEpoch: 213\nEpoch: 214\nEpoch: 215\nEpoch: 216\nEpoch: 217\nEpoch: 218\nEpoch: 219\nEpoch: 220\nEpoch: 221\nEpoch: 222\nEpoch: 223\nEpoch: 224\nEpoch: 225\nEpoch: 226\nEpoch: 227\nEpoch: 228\nEpoch: 229\nEpoch: 230\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}